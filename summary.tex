\documentclass[11pt,landscape,a4paper,fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning,arrows,fit,calc,graphs,graphs.standard}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage[top=3mm,bottom=4mm,left=3mm,right=3mm]{geometry}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{microtype}
\usepackage{mathtools}
\usepackage{ccicons}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{amssymb}
\usepackage[neveradjust]{paralist}
\usepackage[shortlabels]{enumitem}
\usepackage{bbm}
\usepackage{listings}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}
% \usepackage{ETbb}
% \usepackage[sc]{mathpazo}
\usepackage{algpseudocode}
\usepackage{inconsolata}
\usepackage{physics}
\usepackage{bm}
\usepackage{svg}
\usepackage{wrapfig}

\providetoggle{showextratext}
\settoggle{showextratext}{false}

\setlength{\columnsep}{2mm}

\setlist{topsep=0pt, leftmargin=*, noitemsep, topsep=0pt,parsep=0pt,partopsep=0pt}

\newcommand{\extratext}[1]{\iftoggle{showextratext}{#1}{}}

\newcommand*{\tran}{^{\mathsf{T}}} % (DIN) EN ISO 80000-2:2013
\newcommand{\kl}[2]{D_{\mathrm{KL}}(#1\lVert#2)}
\newcommand{\js}[2]{D_{\mathrm{JS}}(#1\lVert#2)}

\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\softmax}{\mathrm{Softmax}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\exp}{\mathrm{exp}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\setdefaultleftmargin{0.5cm}{}{}{}{}{}

\let\bar\overline

\definecolor{myblue}{cmyk}{1,.72,0,.38}
\definecolor{myorange}{cmyk}{0,0.5,1,0}
\definecolor{myorange2}{cmyk}{0,0.8,0.8,0}
\definecolor{darkgreen}{cmyk}{0.97,0,1,0.57}

\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

%\everymath\expandafter{\the\everymath \color{myblue}}
%\everydisplay\expandafter{\the\everydisplay \color{myblue}}

\renewcommand{\baselinestretch}{.8}
\pagestyle{empty}

\global\mdfdefinestyle{header}{%
linecolor=gray,linewidth=1pt,%
leftmargin=0mm,rightmargin=0mm,skipbelow=0mm,skipabove=0mm,
}

\newcommand{\header}{
\begin{mdframed}[style=header]
\footnotesize
\sffamily
Cheat sheet\\
Yannick Merkli,~page~\thepage~of~2
\end{mdframed}
}

\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {0pt}%
                                {0.5pt}%x
                                {\color{myorange}\sffamily\small\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{1}{0mm}%
                                {0pt}%
                                {0.1pt}%x
                            	{\color{myorange2}\sffamily\small}}


\makeatother
\setlength{\parindent}{0pt}

\newcommand{\mhl}[1]{\setlength{\fboxsep}{0pt}\colorbox{yellow}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand\iid{\stackrel{\mathclap{\normalfont\mbox{\tiny{iid}}}}{=}}
\lstset{
	basicstyle=\small,
	mathescape
}

\def\myvector#1{\mathbf{#1}}
\def\va{{\myvector{a}}}
\def\vb{{\myvector{b}}}
\def\vc{{\myvector{c}}}
\def\vd{{\myvector{d}}}
\def\ve{{\myvector{e}}}
\def\vf{{\myvector{f}}}
\def\vg{{\myvector{g}}}
\def\vh{{\myvector{h}}}
\def\vi{{\myvector{i}}}
\def\vj{{\myvector{j}}}
\def\vk{{\myvector{k}}}
\def\vl{{\myvector{l}}}
\def\vm{{\myvector{m}}}
\def\vn{{\myvector{n}}}
\def\vo{{\myvector{o}}}
\def\vp{{\myvector{p}}}
\def\vq{{\myvector{q}}}
\def\vr{{\myvector{r}}}
\def\vs{{\myvector{s}}}
\def\vt{{\myvector{t}}}
\def\vu{{\myvector{u}}}
\def\vv{{\myvector{v}}}
\def\vw{{\myvector{w}}}
\def\vx{{\myvector{x}}}
\def\vy{{\myvector{y}}}
\def\vz{{\myvector{z}}}

\def\mymatrix#1{\mathbf{#1}}
\def\mA{{\mymatrix{A}}}
\def\mB{{\mymatrix{B}}}
\def\mC{{\mymatrix{C}}}
\def\mD{{\mymatrix{D}}}
\def\mE{{\mymatrix{E}}}
\def\mF{{\mymatrix{F}}}
\def\mG{{\mymatrix{G}}}
\def\mH{{\mymatrix{H}}}
\def\mI{{\mymatrix{I}}}
\def\mJ{{\mymatrix{J}}}
\def\mK{{\mymatrix{K}}}
\def\mL{{\mymatrix{L}}}
\def\mM{{\mymatrix{M}}}
\def\mN{{\mymatrix{N}}}
\def\mO{{\mymatrix{O}}}
\def\mP{{\mymatrix{P}}}
\def\mQ{{\mymatrix{Q}}}
\def\mR{{\mymatrix{R}}}
\def\mS{{\mymatrix{S}}}
\def\mT{{\mymatrix{T}}}
\def\mU{{\mymatrix{U}}}
\def\mV{{\mymatrix{V}}}
\def\mW{{\mymatrix{W}}}
\def\mX{{\mymatrix{X}}}
\def\mY{{\mymatrix{Y}}}
\def\mZ{{\mymatrix{Z}}}

\begin{document}
	
% \section*{Disclaimer}

\setlength{\columnseprule}{0.1pt}
\begin{multicols*}{4}
    
\section{Generative modelling}

Learn $p_{\mathrm{model}} \approx p_{\mathrm{data}}$, sample from $p_{\mathrm{model}}$.

\begin{itemize}[leftmargin=0.5em]
    \item Explicit density:
    \begin{itemize}[leftmargin=0.3em]
        \item Approximate:
        \begin{itemize}[leftmargin=0.0em]
            \item Variational: VAE, Diffusion
            \item \textcolor{gray}{Markov Chain: Boltzmann machine}
        \end{itemize}
        \item Tractable:
        \begin{itemize}[leftmargin=0.0em]
            \item Autoregressive: FVSBN/NADE/MADE, Pixel\textsc{(c/r)nn}, WaveNet/\textsc{tcn}, Autor. Transf., 
            \item Normalizing Flows
        \end{itemize}
    \end{itemize}
    \item Implicit density:
    \begin{itemize}[leftmargin=0.3em]
        \item Direct: Generative Adversarial Networks
        \item \textcolor{gray}{MC: Generative Stochastic Networks}
    \end{itemize}
\end{itemize}

Autoencoder: $X \textcolor{blue}{\to} Z \textcolor{red}{\to} X$, $\textcolor{red}{g} \circ \textcolor{blue}{f} \approx \mathrm{id}$,
$f$ and $g$ are NNs. Optimal linear autoencoder is PCA.

Undercomplete: $|Z| < |X|$, else overcomplete.
Overcomp. is for denoising, inpainting.

Latent space should be continuious and interpolable.
Autoencoder spaces are neither,
so they are only good for reconstruction.

\section{Variational AutoEncoder (VAE)}

Sample $z$ from prior $p_\theta(z)$, to decode use conditional $p_\theta(x \mid z)$ defined by a NN.

$\kl{P}{Q} \coloneqq \int_x p(x) \log \frac{p(x)}{q(x)} \dd x$: KL divergence,
measure similarity of prob. distr.

$\kl{P}{Q} \neq \kl{Q}{P}, \kl{P}{Q} \geq 0$

Likelihood $p_\theta(x) = \int_z p_\theta(x \mid z) p_\theta(z) \dd z$ is hard to max.,
let enc. NN be $q_\phi(z \mid x)$,
$\log p_\theta(x^{i}) = \textcolor{orange}{\E_z\left[\log p_\theta(x^{i} \mid z)\right]}
- \textcolor{purple}{\kl{q_\phi(z \mid x^{i})}{p_\theta(z)}} + \textcolor{red}{\kl{q_\phi(z \mid x^{i})}{p_\theta(z \mid x^{i})}}$.
\textcolor{red}{Red} is intractable, use $\geq 0$ to ignore it;
\textcolor{orange}{Orange} is reconstruction loss, clusters similar samples;
\textcolor{purple}{Purple} makes posterior close to prior, adds cont. and interp.
$\mathrm{\textcolor{orange}{Orange}} - \mathrm{\textcolor{purple}{Purple}}$ is \textbf{ELBO}, maximize it.

$x \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{blue}{enc}}}} \mu_{z \mid x}, \Sigma_{z \mid x} \xrightarrow{\mathrm{sample}} z \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{red}{dec}}}} \mu_{x \mid z}, \Sigma_{x \mid z}  \xrightarrow{\mathrm{sample}} \hat{x}$

Backprop through sample by reparametr.: $z = \mu + \sigma \epsilon$.
For inference, use $\mu$ directly.

Disentanglement: features should correspond to distinct factors of variation.
Can be done with semi-supervised learning by making $z$ conditionally independent of given features $y$.

\subsection{$\beta$-VAE}

Disentangle by
$\max_{\theta, \phi} \E_x\left[\E_{z \sim q_\phi} \log p_\theta(x \mid z)\right]$ s.t.
$\kl{q_\phi(z \mid x)}{p_\theta(z)} < \delta$, with KKT: $\max \textcolor{orange}{\mathrm{Orange}} - \beta\textcolor{purple}{\mathrm{Purple}}$.

\section{Autoregressive generative models}

Autoregression: use data from the same input variable at previous time steps

Discriminative: $P(Y \mid X)$, generative: $P(X, Y)$, maybe with $Y$ missing.
Sequence models are generative: from $x_i \dots x_{i + k}$ predict $x_{i + k + 1}$.

Tabular approach: $p(\vx) = \prod_i p(x_i \mid \vx_{<i})$, needs $2^{i - 1}$ params.
Independence assumption is too strong.
Let $p_{\theta_i}(x_i \mid \vx_{ < i}) = \operatorname{Bern}(f_i(\vx_{ < i}))$,
where $f_i$ is a NN.
\textbf{Fully Visible Sigmoid Belief Networks}: $f_i = \sigma(\alpha^{(i)}_0 + \bm{\alpha}^{(i)} \vx_{ < i}\tran)$,
complexity $n^2$, but model is linear.

\textbf{Neural Autoregressive Density Estimator}: add hidden layer.
$\vh_i = \sigma(\vb + \mW_{\centerdot, < i} \vx_{ < i})$,
$\hat{x}_i = \sigma(c_i + \mV_{i,\centerdot} \vh_i)$.
Order of $\vx$ can be arbitrary but fixed.
Train by max log-likelihood in $\mathcal{O}(TD)$, can use 2nd order optimizers,
can use \textbf{teacher forcing}: feed GT as previous output.

Extensions: Convolutional; Real-valued: conditionals by mixture of gaussians;
Order-less and deep: one DNN predicts $p(x_k \mid x_{i_1} \dots x_{i_j})$.

\textbf{Masked Autoencoder Distribution Estimator}:
mask out weights s.t. no information flows from $x_d \dots $ to $\hat{x}_d$.
Large hidden layers needed.
Trains as fast as autoencoders, but sampling needs $D$ forward passes.

\textbf{PixelRNN}: generate pixels from corner, dependency on previous pixels is by RNN (LSTM).
\textbf{PixelCNN}: also from corner, but condition by CNN over context region (perceptive field) $ \Rightarrow $ parallelize.
For conditionals use \textul{masked} convolutions.
Channels: model R from context, G from R + cont., B from G + R + cont.
Training is parallel, but inference is sequential $ \Rightarrow $ slow.
Use conv. stacks to mask correctly.

NLL is a natural metric for autoreg. models,
hard to evaluate others.

\textbf{WaveNet}: audio is high-dimensional.
Use dilated convolutions to increase perceptive field with multiple layers.

AR does not work for high res images/video, convert the images into a series of tokens with an AE:
Vector-quantized VAE.
The codebook is a set of vectors.
$x \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{blue}{enc}}}} z \xrightarrow{\mathrm{codebook}} z_q \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{red}{dec}}}} \hat{x}$.

We can run an AR model in the latent space.

\subsection{Attention}

$\vx_t$ is a convex combination of the past steps, with access to all past steps.
For $X \in \R^{T \times D}$: 
$K = XW_K, V = XW_V, Q = XW_Q$.
Check pairwise similarity between query and keys via dot product:
let attention weights be $\bm{\alpha} = \softmax(QK\tran / \sqrt{D})$, $\bm{\alpha} \in \R^{1 \times T}$.
Adding mask $M$ to avoid looking into the future:
\[X = \softmax\left(\frac{(XW_Q)(XW_K)\tran}{\sqrt{D}} + M\right)(XW_V)\]
Multi-head attn. splits $W$ into $h$ heads, then concatenates them.
Positional encoding injects information about the position of the token.
Attn. is $\mathcal{O}(T^2 D)$.

\section{Normalizing Flows}

VAs dont have a tractable likelihood, AR models have no latent space.
Want both.
Change of variable for $x = f(z)$:
$p_x(x) = p_z(f^{-1}(x)) \abs{\det \pdv{f^{-1}(x)}{x}} = p_z(f^{-1}(x)) \abs{\det \pdv{f(z)}{z}}^{-1}$.
Map $Z \to X$ with a deterministic invertible $f_\theta$.
This can be a NN, but computing the determinant is $\mathcal{O}(n^3)$.
If the Jacobian is triangular, the determinant is $\mathcal{O}(n)$.
To do this, add a coupling layer:
\begin{minipage}{0.5\linewidth}
    \[\begin{pmatrix}
        y^A \\
        y^B
    \end{pmatrix} = \begin{pmatrix}
        h(x^A, \beta(x^B)) \\
        x^B
    \end{pmatrix}\]
\end{minipage}%
\hspace{0.3cm}%
\begin{minipage}{0.45\linewidth}
    , where $\beta$ is any model, and $h$ is elementwise.
\end{minipage}
\[\begin{pmatrix}
    x^A \\
    x^B
\end{pmatrix} = \begin{pmatrix}
    h^{-1}(y^A, \beta(y^B)) \\
    y^B
\end{pmatrix}, J = \begin{pmatrix}
    h' & h'\beta' \\
    0 & 1
\end{pmatrix}\]
Stack these for expressivity, $f = f_k \circ \dots f_1$.
$p_x(x) = p_z(f^{-1}(x)) \prod_k \abs{\det \pdv{f_k^{-1}(x)}{x}}$.

Sample $z \sim p_z$ and get $x = f(z)$.

\includesvg[width=\linewidth]{figures/flow.svg}

\begin{wrapfigure}{R}{0.2\linewidth}
    \includesvg[width=\linewidth]{figures/flow-block.svg}
\end{wrapfigure}
\textbullet\ Squeeze: reshape, increase chan.

\textbullet\ ActNorm: batchnorm with init. s.t. output $\sim \mathcal{N}(0, \mI)$.
    $\vy_{i,j} = \vs \odot \vx_{i,j} + \vb$,
    $\vx_{i, j} = (\vy_{i,j} - \vb) / \vs$,
    $\log\det = H \cdot W \cdot \sum_i \log \abs{\vs_i}$: linear.

\textbullet\ $1 \times 1$ conv: permutation along channel dim.
    Init $\mW$ as rand. ortogonal $\in \R^{C \times C}$ with $\det\mW = 1$.
    $\log\det = H \cdot W \cdot \log\abs{\det\mW}$: $\mathcal{O}(C^3)$.
    Faster: $\mW \coloneqq \mP \mL(\mU + \diag(s))$,
    where $\mP$ is a random \underline{fixed} permut. matrix,
    $\mL$ is lower triang. with 1s on diag.,
    $\mU$ is upper triang. with 0s on diag.,
    $\vs$ is a vector.
    Then $\log\det = \sum_i \log \abs{\vs_i}$: $\mathcal{O}(C)$
Conditional coupling: add parameter $\vw$ to $\beta$.

\textbf{SRFlow}: use flows to generate many high-res images from a low-res one.
Adds affine injector between conv. and coupling layers.
$\vh^{n+1} = \exp(\beta^n_{\theta, s}(\vu)) \cdot \vh^n + \beta_{\theta, b}(\vu)$,
$\vh^n = \exp( - \beta^n_{\theta, s}(\vu)) \cdot (\vh^{n+1} - \beta^n_{\theta, b}(\vu))$,
$\log\det = \sum_{i,j,k} \beta^n_{\theta, s}(\vu_{i, j, k})$.

\textbf{StyleFlow}: Take StyleGAN and replace the network $\vz \to \vw$ (aux. latent space)
with a normalizing flow conditioned on attributes.

\textbf{C-Flow}: condition on other normalizing flows: multimodal flows.
Encode original image $\vx_B^1$: $\vz_B^1 = f^{-1}_\phi(\vx_B^1 \mid \vx_A^1)$;
encode extra info (image, segm. map, etc.) $\vx_A^2$: $\vz_A^2 = g^{-1}_\theta(\vx_A^2)$;
generate new image $\vx_B^2$: $\vx_B^2 = f_\phi(\vz_B^1 \mid \vz_A^2)$.

Flows are expensive for training and low res.
The latent distr. of a flow needn't be $\mathcal{N}$.

\section{Generative Adversarial Networks (GANs)}

Log-likelihood is not a good metric. We can have high likelihood with poor quality by mixing in noise and not losing much likelihood; or low likelihood with good quality by remembering input data and having sharp peaks there.

\textbf{Generator} $G : \R^Q \to \R^D$ maps noise $z$ to data,
\textbf{discriminator} $D : \R^D \to [0, 1]$ tries to decide if data is real or fake,
receiving both gen. outputs and training data.
Train $D$ for $k$ steps for each step of $G$.

Training GANs is a min-max process, which are hard to optimize.
$V(G, D) = \E_{\vx \sim p_{\mathrm{d}}} \log(D(\vx)) + \E_{\hat{\vx} \sim p_{\mathrm{m}}} \log(1 - D(\hat{\vx}))$

For $G$ the opt. $D^* = p_{\mathrm{d}}(\vx) / (p_{\mathrm{d}}(\vx) + p_{\mathrm{m}}(\vx))$.

Jensen-Shannon divergence (symmetric):
$\js{p}{q} = \frac{1}{2} \kl{p}{\frac{p + q}{2}} + \frac{1}{2} \kl{p}{\frac{p + q}{2}}$.
Global minimum of $\js{p_{\mathrm{d}}}{p_{\mathrm{m}}}$ is the glob. min. of $V(G, D)$
and $V(G, D^*) = - \log(4)$.

If $G$ and $D$ have enough capacity, at each update step $D$ reaches $D^*$
and $p_{\mathrm{m}}$ improves $V(p_{\mathrm{m}}, D^*) \propto \sup_D \int_{\vx} p_{\mathrm{m}}(\vx) \log( - D(\vx)) \dd \vx$,
then $p_{\mathrm{m}} \to p_{\mathrm{d}}$ by convexity of $V(p_{\mathrm{m}}, D^*)$ wrt. $p_{\mathrm{m}}$.
These assumptions are too strong.

If $D$ is too strong, $G$ has near zero gradients and doesn't learn ($\log'(1 - D(G(z))) \approx 0$).
Use gradient \underline{ascent} on $\log(D(G(z)))$ instead.

Model collapse: $G$ only produces one sample or one class of samples.
Solution: \textbf{unrolling} --- use $k$ previous $D$ for each $G$ update.

DCGAN: pool $\to$ strided convolution, batchnorm, no FC, ReLU for $G$, LeakyReLU for $D$.

Wasserstein GAN: different loss, gradients don't vanish.
Adding gradient penalty for $D$ stabilizes training.
Hierarchical GAN: generate low-res image, then high-res during training.
StyleGAN: learn intermediate latent space $\mathcal{W}$ with FCs,
batchnorm with scale and mean from $\mathcal{W}$, add noise at each layer.

GAN \textbf{inversion}: find $z$ s.t. $G(z) \approx x$ $ \Rightarrow $ manipulate images in latent space, inpainting.
If $G$ predicts image and segmentation mask,
we can use inversion to predict mask for any image, even outside the training distribution.

\subsection{3D GANs}

3D GAN: voxels instead of pixels.
PlatonicGAN: 2D input, 3D output differentiably rendered back to 2D for $D$.

HoloGAN: 3D GAN + 2D superresolution GAN

GRAF: radiance fields more effic. than voxels

GIRAFFE: GRAF + 2D conv. upscale

EG3D: use 3 2D images from StyleGAN for features, project each 3D point to tri-planes.

\subsection{Image Translation}

E.g. sketch $X \to$ image $Y$.
Pix2Pix:
$G : X \to Y$,
$D : X, Y \to [0, 1]$.
GAN loss $+ L_1$ loss between sketch and image.
Needs pairs for training.

CycleGAN: unpaired.
Two GANs $F: X \to Y, G : Y \to X$,
cycle--consistency loss $F \circ G \approx \mathrm{id}; G \circ F \approx \mathrm{id}$
plus GAN losses for $F$ and $G$.

BicycleGAN: add noise input.

Vid2vid: video translation.

\section{Diffusion models}

High quality generations, better diversity, more stable/scalable.

Diffusion (forward) step $q$: adds noise to $\vx_t$ (not learned).
Denoising (reverse) step $p_\theta$: removes noise from $\vx_t$ (learned).

$q(\vx_t \mid \vx_{t-1}) = \mathcal{N}(\sqrt{1 - \beta} \vx_{t-1}, \beta_t \mI)$

$p_\theta(\vx_{t-1} \mid \vx_t) = \mathcal{N}(\mu_\theta(\vx_t, t), \sigma_t^2 \mI)$

$\beta_t$ is the variance schedule (monotone $\uparrow$).
Let $\alpha_t \coloneqq 1 - \beta_t, \overline{\alpha}_t \coloneqq \prod \alpha_i$,
then $q(\vx_t \mid \vx_0) = \mathcal{N}(\sqrt{\overline{\alpha}_t} \vx_0, (1 - \overline{\alpha}_t)\mI)
\Rightarrow \vx_t = \sqrt{\overline{\alpha}_t} \vx_0 + \sqrt{1 - \overline{\alpha}_t}\epsilon$.

Denoising is not tractable naively:
$q(\vx_{t-1} \mid \vx_t) = q(\vx_t \mid \vx_{t-1}) q(\vx_{t-1}) / q(\vx_t)$,
$q(\vx_t) = \int q(\vx_t \mid \vx_0) q(\vx_0) \dd \vx_0$.

Conditioning on $\vx_0$ we get a Gaussian.
Learn model $p_\theta(\vx_{t-1} \mid \vx_t) \approx q(\vx_{t-1} \mid \vx_t, \vx_0)$
by predicting the mean.

$\log p(\vx_0) \geq \E_{q(\vx_{1:T} \mid \vx_0)} \log(p(\vx_{0:T}) / q(\vx_{1:T} \mid \vx_0)) =
\textcolor{orange}{\E_{q(\vx_1 \mid \vx_0)}\log p_\theta(\vx_0 \mid \vx_1)} -
\textcolor{purple}{\kl{q(\vx_T \mid \vx_0)}{p(\vx_T)}} -
\textcolor{blue}{\sum_{t = 2}^T \E_{q(\vx_t \mid \vx_0)} \kl{q(\vx_{t-1} \mid \vx_t, \vx_0)}{p_\theta(\vx_{t-1} \mid \vx_t)}}$,
where \textcolor{orange}{orange} and \textcolor{purple}{purple} are the same as in VAEs,
and \textcolor{blue}{blue} are the extra loss functions.
In a sense VAEs are 1-step diffusion models.

$t$-th denoising is just $\argmin_\theta \frac{1}{2 \sigma_q^2(t)} \norm{\mu_\theta - \mu_q}_2^2$,
so we want $\mu_\theta(\vx_t, t) \approx \mu_q(\vx_t, \vx_0)$.
$\mu_q(\vx_t, \vx_0)$ can be written as
$\frac{1}{\sqrt{\alpha_t}} \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \overline{\alpha}_t} \sqrt{\alpha_t}} \epsilon_0$,
and $\mu_\theta(\vx_t, t) = \frac{1}{\sqrt{\alpha_t}} \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \overline{\alpha}_t} \sqrt{\alpha_t}} \hat{\epsilon}_\theta(\vx_t, t)$,
so the NN learns to predict the added noise.

Training: img $\vx_0, t \sim \mathrm{Unif}(1... T), \epsilon \sim \mathcal{N}(0, \mI)$,
GD on $\nabla_\theta\norm{\epsilon - \epsilon_\theta(\sqrt{\overline{\alpha}_t}\vx_0 + \sqrt{1 - \overline{\alpha}_t}\epsilon, t)}^2$.

Sampling: $\vx_T \sim \mathcal{N}(0, \mI)$, for $t = T$ downto $1$:
$\vz \sim \mathcal{N}(0, I)$ if $t > 1$ else $\vz = 0$;

$\vx_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \overline{\alpha}_t}} \epsilon_\theta(\vx_t, t)) + \sigma_t \vz$.

$\sigma_t^2 = \beta_t$ in practice.
$t$ can be continuious.

\subsection{Conditional generation}

Add input $y$ to the model.

\textbf{ControlNet}: don't retrain model, add layers that add something to block outputs.

\textbf{Guidance}: mix predictions of a conditional and unconditional model,
because conditional models are not diverse.

\subsection{Latent diffusion models}

High-res images are expensive to model.
Predict in latent space, decode with a decoder.

\end{multicols*}

\end{document}