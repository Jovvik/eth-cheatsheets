\documentclass[11pt,landscape,a4paper,fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning,arrows,fit,calc,graphs,graphs.standard}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage[top=3mm,bottom=4mm,left=3mm,right=3mm]{geometry}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{microtype}
\usepackage{mathtools}
\usepackage{ccicons}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{amssymb}
\usepackage[neveradjust]{paralist}
\usepackage[shortlabels]{enumitem}
\usepackage{bbm}
\usepackage{listings}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}
% \usepackage{ETbb}
% \usepackage[sc]{mathpazo}
\usepackage{algpseudocode}
\usepackage{inconsolata}
\usepackage{physics}
\usepackage{bm}
\usepackage{svg}
\usepackage{wrapfig}

\providetoggle{showextratext}
\settoggle{showextratext}{false}

\setlength{\columnsep}{2mm}

\setlist{topsep=0pt, leftmargin=*, noitemsep, topsep=0pt,parsep=0pt,partopsep=0pt}

\newcommand{\extratext}[1]{\iftoggle{showextratext}{#1}{}}

\newcommand*{\tran}{^{\mathsf{T}}} % (DIN) EN ISO 80000-2:2013
\newcommand{\kl}[2]{D_{\mathrm{KL}}(#1\lVert#2)}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\softmax}{\mathrm{Softmax}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\exp}{\mathrm{exp}}

\setdefaultleftmargin{0.5cm}{}{}{}{}{}

\let\bar\overline

\definecolor{myblue}{cmyk}{1,.72,0,.38}
\definecolor{myorange}{cmyk}{0,0.5,1,0}
\definecolor{myorange2}{cmyk}{0,0.8,0.8,0}
\definecolor{darkgreen}{cmyk}{0.97,0,1,0.57}

\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

%\everymath\expandafter{\the\everymath \color{myblue}}
%\everydisplay\expandafter{\the\everydisplay \color{myblue}}

\renewcommand{\baselinestretch}{.8}
\pagestyle{empty}

\global\mdfdefinestyle{header}{%
linecolor=gray,linewidth=1pt,%
leftmargin=0mm,rightmargin=0mm,skipbelow=0mm,skipabove=0mm,
}

\newcommand{\header}{
\begin{mdframed}[style=header]
\footnotesize
\sffamily
Cheat sheet\\
Yannick Merkli,~page~\thepage~of~2
\end{mdframed}
}

\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {0pt}%
                                {0.5pt}%x
                                {\color{myorange}\sffamily\small\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{1}{0mm}%
                                {0pt}%
                                {0.1pt}%x
                            	{\color{myorange2}\sffamily\small}}


\makeatother
\setlength{\parindent}{0pt}

\newcommand{\mhl}[1]{\setlength{\fboxsep}{0pt}\colorbox{yellow}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand\iid{\stackrel{\mathclap{\normalfont\mbox{\tiny{iid}}}}{=}}
\lstset{
	basicstyle=\small,
	mathescape
}

\def\myvector#1{\mathbf{#1}}
\def\va{{\myvector{a}}}
\def\vb{{\myvector{b}}}
\def\vc{{\myvector{c}}}
\def\vd{{\myvector{d}}}
\def\ve{{\myvector{e}}}
\def\vf{{\myvector{f}}}
\def\vg{{\myvector{g}}}
\def\vh{{\myvector{h}}}
\def\vi{{\myvector{i}}}
\def\vj{{\myvector{j}}}
\def\vk{{\myvector{k}}}
\def\vl{{\myvector{l}}}
\def\vm{{\myvector{m}}}
\def\vn{{\myvector{n}}}
\def\vo{{\myvector{o}}}
\def\vp{{\myvector{p}}}
\def\vq{{\myvector{q}}}
\def\vr{{\myvector{r}}}
\def\vs{{\myvector{s}}}
\def\vt{{\myvector{t}}}
\def\vu{{\myvector{u}}}
\def\vv{{\myvector{v}}}
\def\vw{{\myvector{w}}}
\def\vx{{\myvector{x}}}
\def\vy{{\myvector{y}}}
\def\vz{{\myvector{z}}}

\def\mymatrix#1{\mathbf{#1}}
\def\mA{{\mymatrix{A}}}
\def\mB{{\mymatrix{B}}}
\def\mC{{\mymatrix{C}}}
\def\mD{{\mymatrix{D}}}
\def\mE{{\mymatrix{E}}}
\def\mF{{\mymatrix{F}}}
\def\mG{{\mymatrix{G}}}
\def\mH{{\mymatrix{H}}}
\def\mI{{\mymatrix{I}}}
\def\mJ{{\mymatrix{J}}}
\def\mK{{\mymatrix{K}}}
\def\mL{{\mymatrix{L}}}
\def\mM{{\mymatrix{M}}}
\def\mN{{\mymatrix{N}}}
\def\mO{{\mymatrix{O}}}
\def\mP{{\mymatrix{P}}}
\def\mQ{{\mymatrix{Q}}}
\def\mR{{\mymatrix{R}}}
\def\mS{{\mymatrix{S}}}
\def\mT{{\mymatrix{T}}}
\def\mU{{\mymatrix{U}}}
\def\mV{{\mymatrix{V}}}
\def\mW{{\mymatrix{W}}}
\def\mX{{\mymatrix{X}}}
\def\mY{{\mymatrix{Y}}}
\def\mZ{{\mymatrix{Z}}}

\begin{document}
	
% \section*{Disclaimer}

\setlength{\columnseprule}{0.1pt}
\begin{multicols*}{4}
    
\section{Generative modelling}

Learn $p_{\mathrm{model}} \approx p_{\mathrm{data}}$, sample from $p_{\mathrm{model}}$.

\begin{itemize}[leftmargin=0.5em]
    \item Explicit density:
    \begin{itemize}[leftmargin=0.3em]
        \item Approximate:
        \begin{itemize}[leftmargin=0.0em]
            \item Variational: VAE, Diffusion
            \item \textcolor{gray}{Markov Chain: Boltzmann machine}
        \end{itemize}
        \item Tractable:
        \begin{itemize}[leftmargin=0.0em]
            \item Autoregressive: FVSBN/NADE/MADE, Pixel\textsc{(c/r)nn}, WaveNet/\textsc{tcn}, Autor. Transf., 
            \item Normalizing Flows
        \end{itemize}
    \end{itemize}
    \item Implicit density:
    \begin{itemize}[leftmargin=0.3em]
        \item Direct: Generative Adversarial Networks
        \item \textcolor{gray}{MC: Generative Stochastic Networks}
    \end{itemize}
\end{itemize}

Autoencoder: $X \textcolor{blue}{\to} Z \textcolor{red}{\to} X$, $\textcolor{red}{g} \circ \textcolor{blue}{f} \approx \mathrm{id}$,
$f$ and $g$ are NNs. Optimal linear autoencoder is PCA.

Undercomplete: $|Z| < |X|$, else overcomplete.
Overcomp. is for denoising, inpainting.

Latent space should be continuious and interpolable.
Autoencoder spaces are neither,
so they are only good for reconstruction.

\section{Variational AutoEncoder (VAE)}

Sample $z$ from prior $p_\theta(z)$, to decode use conditional $p_\theta(x \mid z)$ defined by a NN.

$\kl{P}{Q} \coloneqq \int_x p(x) \log \frac{p(x)}{q(x)} \dd x$: KL divergence,
measure similarity of prob. distr.

$\kl{P}{Q} \neq \kl{Q}{P}, \kl{P}{Q} \geq 0$

Likelihood $p_\theta(x) = \int_z p_\theta(x \mid z) p_\theta(z) \dd z$ is hard to max.,
let enc. NN be $q_\phi(z \mid x)$,
$\log p_\theta(x^{i}) = \textcolor{orange}{\E_z\left[\log p_\theta(x^{i} \mid z)\right]}
- \textcolor{purple}{\kl{q_\phi(z \mid x^{i})}{p_\theta(z)}} + \textcolor{red}{\kl{q_\phi(z \mid x^{i})}{p_\theta(z \mid x^{i})}}$.
\textcolor{red}{Red} is intractable, use $\geq 0$ to ignore it;
\textcolor{orange}{Orange} is reconstruction loss, clusters similar samples;
\textcolor{purple}{Purple} makes posterior close to prior, adds cont. and interp.
$\mathrm{\textcolor{orange}{Orange}} - \mathrm{\textcolor{purple}{Purple}}$ is \textbf{ELBO}, maximize it.

$x \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{blue}{enc}}}} \mu_{z \mid x}, \Sigma_{z \mid x} \xrightarrow{\mathrm{sample}} z \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{red}{dec}}}} \mu_{x \mid z}, \Sigma_{x \mid z}  \xrightarrow{\mathrm{sample}} \hat{x}$

Backprop through sample by reparametr.: $z = \mu + \sigma \epsilon$.
For inference, use $\mu$ directly.

Disentanglement: features should correspond to distinct factors of variation.
Can be done with semi-supervised learning by making $z$ conditionally independent of given features $y$.

\subsection{$\beta$-VAE}

Disentangle by
$\max_{\theta, \phi} \E_x\left[\E_{z \sim q_\phi} \log p_\theta(x \mid z)\right]$ s.t.
$\kl{q_\phi(z \mid x)}{p_\theta(z)} < \delta$, with KKT: $\max \textcolor{orange}{\mathrm{Orange}} - \beta\textcolor{purple}{\mathrm{Purple}}$.

\section{Autoregressive generative models}

Autoregression: use data from the same input variable at previous time steps

Discriminative: $P(Y \mid X)$, generative: $P(X, Y)$, maybe with $Y$ missing.
Sequence models are generative: from $x_i \dots x_{i + k}$ predict $x_{i + k + 1}$.

Tabular approach: $p(\vx) = \prod_i p(x_i \mid \vx_{<i})$, needs $2^{i - 1}$ params.
Independence assumption is too strong.
Let $p_{\theta_i}(x_i \mid \vx_{ < i}) = \operatorname{Bern}(f_i(\vx_{ < i}))$,
where $f_i$ is a NN.
\textbf{Fully Visible Sigmoid Belief Networks}: $f_i = \sigma(\alpha^{(i)}_0 + \bm{\alpha}^{(i)} \vx_{ < i}\tran)$,
complexity $n^2$, but model is linear.

\textbf{Neural Autoregressive Density Estimator}: add hidden layer.
$\vh_i = \sigma(\vb + \mW_{\centerdot, < i} \vx_{ < i})$,
$\hat{x}_i = \sigma(c_i + \mV_{i,\centerdot} \vh_i)$.
Order of $\vx$ can be arbitrary but fixed.
Train by max log-likelihood in $\mathcal{O}(TD)$, can use 2nd order optimizers,
can use \textbf{teacher forcing}: feed GT as previous output.

Extensions: Convolutional; Real-valued: conditionals by mixture of gaussians;
Order-less and deep: one DNN predicts $p(x_k \mid x_{i_1} \dots x_{i_j})$.

\textbf{Masked Autoencoder Distribution Estimator}:
mask out weights s.t. no information flows from $x_d \dots $ to $\hat{x}_d$.
Large hidden layers needed.
Trains as fast as autoencoders, but sampling needs $D$ forward passes.

\textbf{PixelRNN}: generate pixels from corner, dependency on previous pixels is by RNN (LSTM).
\textbf{PixelCNN}: also from corner, but condition by CNN over context region (perceptive field) $ \Rightarrow $ parallelize.
For conditionals use \textul{masked} convolutions.
Channels: model R from context, G from R + cont., B from G + R + cont.
Training is parallel, but inference is sequential $ \Rightarrow $ slow.
Use conv. stacks to mask correctly.

NLL is a natural metric for autoreg. models,
hard to evaluate others.

\textbf{WaveNet}: audio is high-dimensional.
Use dilated convolutions to increase perceptive field with multiple layers.

AR does not work for high res images/video, convert the images into a series of tokens with an AE:
Vector-quantized VAE.
The codebook is a set of vectors.
$x \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{blue}{enc}}}} z \xrightarrow{\mathrm{codebook}} z_q \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{red}{dec}}}} \hat{x}$.

We can run an AR model in the latent space.

\subsection{Attention}

$\vx_t$ is a convex combination of the past steps, with access to all past steps.
For $X \in \R^{T \times D}$: 
$K = XW_K, V = XW_V, Q = XW_Q$.
Check pairwise similarity between query and keys via dot product:
let attention weights be $\bm{\alpha} = \softmax(QK\tran / \sqrt{D})$, $\bm{\alpha} \in \R^{1 \times T}$.
Adding mask $M$ to avoid looking into the future:
\[X = \softmax\left(\frac{(XW_Q)(XW_K)\tran}{\sqrt{D}} + M\right)(XW_V)\]
Multi-head attn. splits $W$ into $h$ heads, then concatenates them.
Positional encoding injects information about the position of the token.
Attn. is $\mathcal{O}(T^2 D)$.

\section{Normalizing Flows}

VAs dont have a tractable likelihood, AR models have no latent space.
Want both.
Change of variable for $x = f(z)$:
$p_x(x) = p_z(f^{-1}(x)) \abs{\det \pdv{f^{-1}(x)}{x}} = p_z(f^{-1}(x)) \abs{\det \pdv{f(z)}{z}}^{-1}$.
Map $Z \to X$ with a deterministic invertible $f_\theta$.
This can be a NN, but computing the determinant is $\mathcal{O}(n^3)$.
If the Jacobian is triangular, the determinant is $\mathcal{O}(n)$.
To do this, add a coupling layer:
\begin{minipage}{0.5\linewidth}
    \[\begin{pmatrix}
        y^A \\
        y^B
    \end{pmatrix} = \begin{pmatrix}
        h(x^A, \beta(x^B)) \\
        x^B
    \end{pmatrix}\]
\end{minipage}%
\hspace{0.3cm}%
\begin{minipage}{0.45\linewidth}
    , where $\beta$ is any model, and $h$ is elementwise.
\end{minipage}
\[\begin{pmatrix}
    x^A \\
    x^B
\end{pmatrix} = \begin{pmatrix}
    h^{-1}(y^A, \beta(y^B)) \\
    y^B
\end{pmatrix}, J = \begin{pmatrix}
    h' & h'\beta' \\
    0 & 1
\end{pmatrix}\]
Stack these for expressivity, $f = f_k \circ \dots f_1$.
$p_x(x) = p_z(f^{-1}(x)) \prod_k \abs{\det \pdv{f_k^{-1}(x)}{x}}$.

Sample $z \sim p_z$ and get $x = f(z)$.

\includesvg[width=\linewidth]{figures/flow.svg}

\begin{wrapfigure}{R}{0.2\linewidth}
    \includesvg[width=\linewidth]{figures/flow-block.svg}
\end{wrapfigure}
\textbullet\ Squeeze: reshape, increase chan.

\textbullet\ ActNorm: batchnorm with init. s.t. output $\sim \mathcal{N}(0, \mI)$.
    $\vy_{i,j} = \vs \odot \vx_{i,j} + \vb$,
    $\vx_{i, j} = (\vy_{i,j} - \vb) / \vs$,
    $\log\det = H \cdot W \cdot \sum_i \log \abs{\vs_i}$: linear.

\textbullet\ $1 \times 1$ conv: permutation along channel dim.
    Init $\mW$ as rand. ortogonal $\in \R^{C \times C}$ with $\det\mW = 1$.
    $\log\det = H \cdot W \cdot \log\abs{\det\mW}$: $\mathcal{O}(C^3)$.
    Faster: $\mW \coloneqq \mP \mL(\mU + \diag(s))$,
    where $\mP$ is a random \underline{fixed} permut. matrix,
    $\mL$ is lower triang. with 1s on diag.,
    $\mU$ is upper triang. with 0s on diag.,
    $\vs$ is a vector.
    Then $\log\det = \sum_i \log \abs{\vs_i}$: $\mathcal{O}(C)$
Conditional coupling: add parameter $\vw$ to $\beta$.

\textbf{SRFlow}: use flows to generate many high-res images from a low-res one.
Adds affine injector between conv. and coupling layers.
$\vh^{n+1} = \exp(\beta^n_{\theta, s}(\vu)) \cdot \vh^n + \beta_{\theta, b}(\vu)$,
$\vh^n = \exp( - \beta^n_{\theta, s}(\vu)) \cdot (\vh^{n+1} - \beta^n_{\theta, b}(\vu))$,
$\log\det = \sum_{i,j,k} \beta^n_{\theta, s}(\vu_{i, j, k})$.

\textbf{StyleFlow}: Take StyleGAN and replace the network $\vz \to \vw$ (aux. latent space)
with a normalizing flow conditioned on attributes.

\textbf{C-Flow}: condition on other normalizing flows: multimodal flows.
Encode original image $\vx_B^1$: $\vz_B^1 = f^{-1}_\phi(\vx_B^1 \mid \vx_A^1)$;
encode extra info (image, segm. map, etc.) $\vx_A^2$: $\vz_A^2 = g^{-1}_\theta(\vx_A^2)$;
generate new image $\vx_B^2$: $\vx_B^2 = f_\phi(\vz_B^1 \mid \vz_A^2)$.

Flows are expensive for training and low res.
The latent distr. of a flow needn't be $\mathcal{N}$.

\end{multicols*}

\end{document}